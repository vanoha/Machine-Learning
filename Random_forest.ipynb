{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Random forest.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNSMVv7BTbdNwGSyOAjw1Bd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vanoha/Machine-Learning/blob/main/Random_forest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veekMy8WRjBi"
      },
      "source": [
        "## Построение дерева"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYkVwAFiUHXj"
      },
      "source": [
        "Опишем жадный алгоритм построения бинарного дерева решений:\n",
        "1. Начинаем со всей обучающей выборки $X$, которую помещаем в корень $R_1$. \n",
        "2. Задаём функционал качества $Q(X, j, t)$ и критерий остановки. \n",
        "3. Запускаем построение из корня: $SplitNode(1, R_1)$\n",
        "\n",
        "Функция $SplitNode(m, R_m)$\n",
        "1. Если выполнен критерий остановки, то выход.\n",
        "2. Находим наилучший с точки зрения $Q$ предикат: $j, t$: $[x_j<t]$\n",
        "3. Помещаем предикат в вкршину и получаем с его помощью разбиение $X$ на две части: $R_{left} = \\lbrace x|x_j<t \\rbrace$ и $R_{right} = \\lbrace x|x_j \\geqslant t \\rbrace$\n",
        "4. Поместим $R_{left}$ и $R_{right}$ соответсвенно в левое и правое поддерево.\n",
        "5. Рекурсивно повторяем $SplitNode(left, R_{left})$ и $SplitNode(right, R_{right})$.\n",
        "\n",
        "В конце поставим в соответствие каждому листу ответ. Для задачи классификации - это самый частый среди объектов класс или вектор с долями классов (можно интерпретировать как вероятности):\n",
        "$$ c_v = \\arg \\max_{k\\in Y} \\sum_{(x_i,y_i) \\in R_v} [y_i=k]  $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9P6FsdBog4Ai"
      },
      "source": [
        "## Функционал качества для деревьев решений\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "c779zJSrm_3x"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VAKO0aykGBD"
      },
      "source": [
        "Энтропия Шеннона для системы с N возможными состояниями определяется по формуле:\n",
        "$$H = - \\sum_{i=0}^{N} p_i\\log_2p_i $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5582B-1Fn2bw"
      },
      "source": [
        "где $p_i$ – вероятности нахождения системы в $i$-ом состоянии. \n",
        "\n",
        "Это очень важное понятие теории информации, которое позволяет оценить количество информации (степень хаоса в системе). Чем выше энтропия, тем менее упорядочена система и наоборот. С помощью энтропии мы формализуем функционал качества для разделение выборки (для задачи классификации)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbcMUd7bvk05"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import random\n",
        "from pprint import pprint"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AdLxP9CowTm"
      },
      "source": [
        "Код для расчёта энтропии:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mT8Jq8Av2sM"
      },
      "source": [
        "def entropy(y):\n",
        "    \n",
        "    _, counts = np.unique(y, return_counts=True)\n",
        "\n",
        "    probabilities = counts / counts.sum()\n",
        "    entropy = sum(probabilities * -np.log2(probabilities))\n",
        "     \n",
        "    return entropy"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xk9etb2vo7fK"
      },
      "source": [
        "Здесь $y$ - это массив значений целевой переменной"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07TCw0USzLus"
      },
      "source": [
        "Энтропия – по сути степень хаоса (или неопределенности) в системе. Уменьшение энтропии называют приростом информации (information gain, IG).\n",
        "\n",
        "Обочначим $R_v$ - объекты, которые нужно разделить в помощью предиката в вершине $v$. Запишем формулу для расчёта информационного прироста:\n",
        "$$ Q = IG = H(R_v) - (H(R_{left})+H(R_{right}))$$\n",
        "\n",
        "На каждом шаге нам нужно максимизировать этот функционал качества. Как это делать? Например, так можно перебрать $t$ для выбранного $j$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trEWHDoXg_p9"
      },
      "source": [
        "Предыдущая версия формулы прироста информации слишком упрощена. В работе необходимо использовать более устойчивую формулу, которая учитывает не только энтропию подмножеств, но и их размер. \n",
        "\n",
        "$$ Q = IG = H(R_v) - \\Big (\\frac{|R_{left}|} {|R_{v}|} H(R_{left})+ \\frac{|R_{right}|} {|R_{v}|} H(R_{right})\\Big)$$\n",
        "\n",
        "где, $|R_{v}|$, $|R_{left}|$ и $|R_{right}|$ - количество элементов в соответствующих множествах."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xmN6V_N1xBr"
      },
      "source": [
        "\n",
        "### Задание 4.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWFHZScF2CBF"
      },
      "source": [
        "Реализуйте алгоритм построения дерева. Должны быть отдельные функции (методы) для расчёта энтропии (уже есть), для разделения дерева (используйте `pandas`), для подсчёта функционала качества $IG$, для выбора наилучшего разделения (с учетом признакоd и порогов), для проверки критерия остановки.\n",
        "\n",
        "Для набора данных `iris` реализуйте алгоритм и минимум три из разными критерия остановки из перечисленных ниже:\n",
        "* максимальной глубины дерева = 5\n",
        "* минимального числа объектов в листе = 5\n",
        "* максимальное количество листьев в дереве = 5\n",
        "* purity (остановка, если все объекты в листе относятся к одному классу)\n",
        "\n",
        "Реализуйте функцию `predict` (на вход функции подаётся датафрейм с объектами)\n",
        "\n",
        "Оцените точность каждой модели с помощью метрики точность (`from sklearn.metrics import accuracy_score` или реализовать свою)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POSSGAzWRdkk"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "class DecisionTree:\n",
        "    class Node:\n",
        "        left = None\n",
        "        right = None\n",
        "        best_t = None\n",
        "        feature = None\n",
        "        target = None\n",
        "\n",
        "    def _entropy_(self, part):\n",
        "        _, counts = np.unique(part, return_counts = True)\n",
        "\n",
        "        probabilities = counts / counts.sum()\n",
        "        entropy = sum(probabilities * -np.log2(probabilities))\n",
        "\n",
        "        return entropy\n",
        "\n",
        "    def _IG_(self, X, feature, t):\n",
        "        part1 = X[X[feature] < t]\n",
        "        part2 = X[X[feature] >= t]\n",
        "        if len(part1) == 0 or len(part2) == 0:\n",
        "            return float('-inf'), [], []\n",
        "        size = len(X[self.target])\n",
        "        ig = self._entropy_(X[self.target]) - (\n",
        "                len(part1) / size * self._entropy_(part1[self.target]) + len(part2) / size * self._entropy_(\n",
        "            part2[self.\n",
        "                target]))\n",
        "        return ig, part1, part2\n",
        "\n",
        "    def __init__(self, max_depth = 5, max_leaf_nodes = 5, min_weight_fraction_leaf = 5):\n",
        "        self.max_depth = max_depth\n",
        "        self.max_leaf_nodes = max_leaf_nodes\n",
        "        self.min_weight_fraction_leaf = min_weight_fraction_leaf\n",
        "\n",
        "    def fit(self, X: pd.DataFrame, y: str):\n",
        "        self.target = y\n",
        "        self.root = self.get_node(X)\n",
        "\n",
        "    def get_node(self, X: pd.DataFrame, current_depth = 0):\n",
        "\n",
        "        if len(np.unique(X[self.target])) == 1 or current_depth == self.max_depth or len(\n",
        "                X) <= self.min_weight_fraction_leaf:\n",
        "            node = self.Node()\n",
        "            node.target = X[self.target].value_counts().index[0]\n",
        "            return node\n",
        "\n",
        "        features = list(X.columns)\n",
        "        features.remove(self.target)\n",
        "        best_feature = features[0]\n",
        "        best_IG = float('-inf')\n",
        "        best_t = 0\n",
        "        best_p1 = pd.DataFrame()\n",
        "        best_p2 = pd.DataFrame()\n",
        "        node = self.Node()\n",
        "        \n",
        "        for feature in features:\n",
        "            grid = np.linspace(X[feature].min(), X[feature].max(), 10)\n",
        "            for t in grid:\n",
        "                IG, p1, p2 = self._IG_(X, feature, t)\n",
        "                if IG > best_IG:\n",
        "                    best_p1 = p1\n",
        "                    best_p2 = p2\n",
        "                    best_t, best_IG, best_feature = t, IG, feature\n",
        "\n",
        "        node.left = self.get_node(best_p1, current_depth + 1)\n",
        "        node.right = self.get_node(best_p2, current_depth + 1)\n",
        "        node.best_t = best_t\n",
        "        node.feature = best_feature\n",
        "        return node\n",
        "\n",
        "    def predict(self, X: pd.DataFrame):\n",
        "        y = []\n",
        "        for i in range(len(X)):\n",
        "            elem = X.iloc[i]\n",
        "            y.append(self.DFS(self.root, elem))\n",
        "        return y\n",
        "\n",
        "    def DFS(self, node: Node, x):\n",
        "        if node.left is None and node.right is None:\n",
        "            return node.target\n",
        "        if x[node.feature] < node.best_t:\n",
        "            if node.left is None:\n",
        "                return node.target\n",
        "            return self.DFS(node.left, x)\n",
        "        else:\n",
        "            if node.right is None:\n",
        "                return node.target\n",
        "            return self.DFS(node.right, x)\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inFV3_hrRyVu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6e2a6e6-1cfc-461d-f69f-a526ff869367"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import datasets\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "\n",
        "df = pd.DataFrame(data = np.c_[iris['data'], iris['target']],\n",
        "                  columns = iris['feature_names'] + ['target'])\n",
        "\n",
        "df['species'] = pd.Categorical.from_codes(iris.target, iris.target_names)\n",
        "df.drop(['target'], axis = 1, inplace = True)\n",
        "df = df.sample(frac = 1)\n",
        "\n",
        "df_train = df.head(100)\n",
        "df_test = df.tail(50)\n",
        "dt = DecisionTree()\n",
        "dt.fit(df_train, 'species')\n",
        "predict = dt.predict(df_test.drop(['species'], axis = 1))\n",
        "\n",
        "k = 0\n",
        "for i, elem in enumerate(list(df_test['species'])):\n",
        "    if elem == predict[i]:\n",
        "        k += 1\n",
        "\n",
        "dtc = DecisionTreeClassifier()\n",
        "dtc.fit(df_train.drop(['species'], axis = 1).to_numpy(), df_train['species'].to_numpy())\n",
        "prd = dtc.predict(df_test.drop(['species'], axis = 1).to_numpy())\n",
        "\n",
        "print(\"схожесть с готовым: \", accuracy_score(predict, prd))\n",
        "print(\"правильность ответов: \", k / len(df_test))\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "схожесть с готовым:  1.0\n",
            "правильность ответов:  0.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkyCjLcy_CTM"
      },
      "source": [
        "##  Случайный лес"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fKZe1FyRgCa"
      },
      "source": [
        "Опишем алгоритм случайный лес (*random forest*) и попутно разберём основные идеи:\n",
        "\n",
        "1. Зададим $N$ - число деревьев в лесу.\n",
        "2. Для каждого $n$ из $N$ сгенерируем свою выборку $X_n$. Пусть $m$ - это количество объектов в $X$. При генерации каждой $X_n$ мы будем брать объекты $m$ раз с возвращением. То есть один и тот же объект может попасть в выборку несколько раз, а какие-то объекты не попадут. (Этот способ назвается бутстрап).\n",
        "3. По каждой $X_n$ построим решающее дерево $b_n$. Обычно стараются делать глубокие деревья. В качестве критериев остановки можно использовать `max_depth` или `min_samples_leaf` (например, пока в каждом листе не окажется по одному объекту). При каждом разбиении сначала выбирается $k$ (эвристика $k = \\sqrt d$, где $d$ - это число признаков объектов из выборки $X$) случайных признаков из исходных, и оптимальное разделение выборки ищется только среди них. Обратите внимание, что мы не выбрасываем оставшиеся признаки!\n",
        "4. Итоговый алгоритм будет представлять собой результат голосования (для классификации) и среднее арифметическое (для регрессии). Модификация алгоритма предполагает учёт весов каждого отдельного слабого алгоритма в ансамбле, но в этом особо нет смысла.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJBQ8lc0WyrN"
      },
      "source": [
        "### Задание 4.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y594Jn04ZTCm"
      },
      "source": [
        "В качестве набора данных используйте: https://www.kaggle.com/mathchi/churn-for-bank-customers\n",
        "\n",
        "Там есть описание и примеры работы с этими данными. Если кратко, речь идёт про задачу прогнозирования оттока клиентов. Есть данные о 10 тысячах клиентов банка, часть из которых больше не являются клиентами."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be_mLbdVW2oG"
      },
      "source": [
        "Используя либо свою реализацию, либо  `DecisionTreeClassifier` с разными настройками из `sklearn.tree` реализуйте алгоритм \"случайный лес\". \n",
        "\n",
        "Найдите наилучшие гиперпараметры этого алгоритма: количество деревьев, критерий остановки, функционал качества, минимальное количество объектов в листьях и другие.\n",
        "\n",
        "Нельзя использовать готовую реализацию случайного леса из `sklearn`.\n",
        "\n",
        "В подобных задачах очень важна интерпретируемость алгоритма. Попытайтесь оценить информативность признаков, т.е. ответить а вопрос, значения каких признаков являются самыми важными индикаторами того, что банк потеряет клиента."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dzl_k19SSTil",
        "outputId": "556b867d-bc98-46da-9cad-686638bdd164"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import random\n",
        "pd.options.mode.chained_assignment = None  # default='warn'\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "df = pd.read_csv(\"churn.csv\", index_col='RowNumber')\n",
        "df.drop(['CustomerId', 'Surname'], axis = 1, inplace = True)\n",
        "df['Gender'] = pd.get_dummies(df['Gender'], drop_first=True)\n",
        "Geography_dumies = pd.get_dummies(df['Geography'], drop_first=True)\n",
        "df = df.drop(columns=['Geography'])\n",
        "df = pd.concat([df, Geography_dumies], axis=1)\n",
        "\n",
        "X_train, X_test = train_test_split(df, random_state = 42, test_size = 0.3)\n",
        "y_test = X_test['Exited']\n",
        "X_test.drop(['Exited'], axis=1, inplace=True)\n",
        "\n",
        "N = 10\n",
        "m = 6000\n",
        "trees = [(DecisionTreeClassifier(max_depth=random.randint(3, 10)), X_train.sample(n=m)) for i in range(N)]\n",
        "y_pred = []\n",
        "\n",
        "for tree, data in trees:\n",
        "    y_train = data['Exited']\n",
        "    X_train = data.drop(['Exited'], axis = 1)\n",
        "    X_train = X_train.sample(n = int((len(X_train.columns)) / 2), axis = 1)\n",
        "    tree.fit(X_train, y_train)\n",
        "    X_test_current = pd.DataFrame(X_test, columns = list(X_train.columns))\n",
        "    y_pred.append(tree.predict(X_test_current))\n",
        "\n",
        "res = []\n",
        "for i in range(len(y_pred[0])):\n",
        "    l = []\n",
        "    for j in range(len(y_pred)):\n",
        "        l.append(y_pred[j][i])\n",
        "    res.append(np.bincount(l).argmax())\n",
        "\n",
        "print(accuracy_score(y_test, res))\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8076666666666666\n"
          ]
        }
      ]
    }
  ]
}